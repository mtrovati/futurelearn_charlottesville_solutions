{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f34626e",
   "metadata": {},
   "source": [
    "# Analysis of Tweets regarding Charlottesville Rally\n",
    "\n",
    "\"*The Unite the Right rally was a white supremacist rally that took place in Charlottesville, Virginia, from August 11 to 12, 2017. Far-right groups participated, including self-identified members of the alt-right,neo-Confederates,neo-fascists, white nationalists, neo-Nazis,and various right-wing militias*\" \n",
    "(from Wikipedia: https://en.wikipedia.org/wiki/Unite_the_Right_rally).\n",
    "\n",
    "The dataset `aug15_sample.csv` contains tweets shared regarding this event."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469b59be",
   "metadata": {},
   "source": [
    "Run the following (after un-commenting them) if your installation of Python does not have Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec4ce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-plaintiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "tweets = pd.read_csv('aug15_sample.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3188af24",
   "metadata": {},
   "source": [
    "Use the '`.head()`' `Pandas` function to display the first entries of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-rover",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b74122f",
   "metadata": {},
   "source": [
    "Let's remove stopwords, such as characters that do not hold any specific meaning. These include punctuations, brackets etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_N = 30\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "RE_stopwords = r'\\b(?:{})\\b'.format('|'.join(stopwords))\n",
    "words = (tweets['full_text']\n",
    "           .str.lower()\n",
    "           .replace([r'\\|', RE_stopwords, r\"(&amp)|,|;|\\\"|\\.|\\?|â€™|!|'|:|-|\\\\|/|https\"], [' ', ' ', ' '], regex=True)\n",
    "           .str.cat(sep=' ')\n",
    "           .split()\n",
    ")\n",
    "\n",
    "rslt = pd.DataFrame(Counter(words).most_common(top_N),\n",
    "                    columns=['Word', 'Frequency']).set_index('Word')\n",
    "\n",
    "rslt = rslt.iloc[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c81757",
   "metadata": {},
   "source": [
    "What is `rslt` ? Print it and display its `type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbf4389",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadaebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(rslt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e4669c",
   "metadata": {},
   "source": [
    "The below cell will display the word frequency values, using the Python module `seaborn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-rebel",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = [30.0, 20.0]\n",
    "ax = sns.barplot(y=rslt.index, x='Frequency', data=rslt)\n",
    "ax.set_xlabel(\"Frequency\",fontsize=25)\n",
    "ax.set_ylabel(\"Words\",fontsize=25)\n",
    "ax.tick_params(labelsize=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90217667",
   "metadata": {},
   "source": [
    "Research the Python module `seaborn`, and try different visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f450c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25496208",
   "metadata": {},
   "source": [
    "Now, let's identify the different hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-reunion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hashtags = hashtags[~hashtags.isnull()]\n",
    "tags = (tweets['hashtags']\n",
    "           .str.lower()\n",
    "           .str.cat(sep=' ')\n",
    "           .split()\n",
    ")\n",
    "\n",
    "hashtgs = pd.DataFrame(Counter(tags).most_common(top_N),\n",
    "                    columns=['Hashtags', 'Frequency']).set_index('Hashtags')\n",
    "hashtgs = hashtgs.iloc[1:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf2d249",
   "metadata": {},
   "source": [
    "Investigate the variable type of `hashtgs` and print its content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e060d2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(hashtgs))\n",
    "print(hashtgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57781fa7",
   "metadata": {},
   "source": [
    "Modify the above code to visualise the `hashtgs` values using Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-instruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "ax = sns.barplot(y=hashtgs.index, x='Frequency', data=hashtgs)\n",
    "ax.set_xlabel(\"Frequency\",fontsize=25)\n",
    "ax.set_ylabel(\"Hashtag\",fontsize=25)\n",
    "ax.tick_params(labelsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-upgrade",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['created_at'] = pd.to_datetime(tweets['created_at'])\n",
    "tweets = tweets.set_index('created_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-shuttle",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_timestamp = tweets[['id']]\n",
    "tweet_volume = tweets_timestamp.resample('10min').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acc2483",
   "metadata": {},
   "source": [
    "Answer the following questions:\n",
    "\n",
    "* What does `resample('10min')` do?\n",
    "* What does `count()` do?\n",
    "* Describe the difference between `tweets_timestamp` and `tweet_volume` by visualising them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bc7346",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5345c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweet_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.pointplot(x=tweet_volume.index, y='id', data=tweet_volume)\n",
    "ax.set_xlabel(\"Timestamp of tweets\",fontsize=30)\n",
    "ax.set_ylabel(\"Number of tweets\",fontsize=30)\n",
    "\n",
    "ax.tick_params(labelsize=25)\n",
    "\n",
    "for item in ax.get_xticklabels():\n",
    "    item.set_rotation(90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f95a32",
   "metadata": {},
   "source": [
    "Let's now find the most influential tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-imaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "influential = tweets[['user_name', 'followers_count']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65373e4",
   "metadata": {},
   "source": [
    "Print the first entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7b1139",
   "metadata": {},
   "outputs": [],
   "source": [
    "influential.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7252a64",
   "metadata": {},
   "source": [
    "Let's now group the above in *ascending* order using the following commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70330f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "influential = influential.sort_values('followers_count', ascending=False)\n",
    "influential.groupby('user_name').first().sort_values(by='followers_count', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-commons",
   "metadata": {},
   "source": [
    "These are the users who tweeted the most during the last 3 hours. \n",
    "\n",
    "Now, print the top 20 people and number of tweets they have tweeted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['screen_name'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-dressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering algorithms \n",
    "# from http://ahmedbesbes.com/how-to-mine-newsfeed-data-and-extract-interactive-insights-in-python.html\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "# nltk for nlp\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# list of stopwords like articles, preposition\n",
    "stop = set(stopwords.words('english'))\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "\n",
    "def tokenizer(text):\n",
    "    try:\n",
    "        tokens_ = [word_tokenize(sent) for sent in sent_tokenize(text)]\n",
    "        \n",
    "        tokens = []\n",
    "        for token_by_sent in tokens_:\n",
    "            tokens += token_by_sent\n",
    "\n",
    "        tokens = list(filter(lambda t: t.lower() not in stop, tokens))\n",
    "        tokens = list(filter(lambda t: t not in punctuation, tokens))\n",
    "        tokens = list(filter(lambda t: t not in [u\"'s\", u\"n't\", u\"...\", u\"''\", u'``', u'amp', u'https',\n",
    "                                                u'via', u\"'re\"], tokens))\n",
    "        filtered_tokens = []\n",
    "        for token in tokens:\n",
    "            if re.search('[a-zA-Z]', token):\n",
    "                filtered_tokens.append(token)\n",
    "\n",
    "        filtered_tokens = list(map(lambda token: token.lower(), filtered_tokens))\n",
    "\n",
    "        return filtered_tokens\n",
    "    except Error as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-guide",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['tokens'] = tweets['full_text'].map(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-export",
   "metadata": {},
   "outputs": [],
   "source": [
    "for full_text, tokens in zip(tweets['full_text'].head(5), tweets['tokens'].head(5)):\n",
    "    print('full text:', full_text)\n",
    "    print('tokens:', tokens)\n",
    "    print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-jordan",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# min_df is minimum number of documents that contain a term t\n",
    "# max_features is maximum number of unique tokens (across documents) that we'd consider\n",
    "# TfidfVectorizer preprocesses the descriptions using the tokenizer we defined above\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=10, max_features=10000, tokenizer=tokenizer, ngram_range=(1, 2))\n",
    "vz = vectorizer.fit_transform(list(tweets['full_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-developer",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "tfidf = pd.DataFrame(columns=['tfidf']).from_dict(dict(tfidf), orient='index')\n",
    "tfidf.columns = ['tfidf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-envelope",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.tfidf.hist(bins=50, figsize=(15,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-denmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.sort_values(by=['tfidf'], ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-mother",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "num_clusters = 10\n",
    "kmeans_model = MiniBatchKMeans(n_clusters=num_clusters, init='k-means++', n_init=1, \n",
    "                         init_size=1000, batch_size=1000, verbose=False, max_iter=1000)\n",
    "kmeans = kmeans_model.fit(vz)\n",
    "kmeans_clusters = kmeans.predict(vz)\n",
    "kmeans_distances = kmeans.transform(vz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-design",
   "metadata": {},
   "source": [
    "I used k-means clustering algorithms to generate a list of words that appear frequently together, and the results are shown above.\n",
    "You can see that there is a thread of conversation that we couldnâ€™t detect from the word frequency list. One example of this is Cluster #3, where a pocket of people expressed their displeasure with CNN coverage. K-means clustering is surely a great way to complement our word frequency tally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-america",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    aux = ''\n",
    "    for j in sorted_centroids[i, :10]:\n",
    "        aux += terms[j] + ' | '\n",
    "    print(aux)\n",
    "    print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-subject",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
